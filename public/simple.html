<!DOCTYPE html>
<html>
<head>
    <title>OpenAI Realtime API Test</title>
    <style>
        body { 
            font-family: Arial; 
            max-width: 800px; 
            margin: 50px auto; 
            padding: 20px;
        }
        button { 
            padding: 15px 30px; 
            font-size: 18px; 
            margin: 10px;
            cursor: pointer;
        }
        #messages { 
            border: 1px solid #ccc; 
            padding: 20px; 
            height: 400px; 
            overflow-y: auto;
            margin: 20px 0;
            background: #f5f5f5;
        }
        .message { 
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
        }
        .user { background: #e3f2fd; }
        .assistant { background: #fff; }
        #status { 
            font-weight: bold; 
            margin: 20px 0;
            padding: 10px;
            background: #fffde7;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <h1>OpenAI Realtime API Test</h1>
    
    <div id="status">Click Connect to start</div>
    
    <button id="connectBtn" onclick="connect()">Connect</button>
    <button id="pushToTalkBtn" onmousedown="startRecording()" onmouseup="stopRecording()" disabled>
        Hold to Talk
    </button>
    
    <div id="messages"></div>

    <script>
        const API_KEY = 'YOUR_OPENAI_API_KEY';
        
        let pc = null;
        let dc = null;
        let audioContext = null;
        let source = null;
        let processor = null;

        function log(message) {
            console.log(message);
            document.getElementById('status').textContent = message;
        }

        function addMessage(role, text) {
            const div = document.createElement('div');
            div.className = 'message ' + role;
            div.textContent = `${role}: ${text}`;
            document.getElementById('messages').appendChild(div);
            document.getElementById('messages').scrollTop = document.getElementById('messages').scrollHeight;
        }

        async function connect() {
            log('Connecting...');
            document.getElementById('connectBtn').disabled = true;

            try {
                // Create peer connection
                pc = new RTCPeerConnection();
                
                // Add audio transceiver
                pc.addTransceiver('audio', { direction: 'sendrecv' });

                // Create data channel for events
                dc = pc.createDataChannel('oai-events', { ordered: true });
                
                dc.onopen = () => {
                    log('Connected! Hold the button to talk.');
                    document.getElementById('pushToTalkBtn').disabled = false;
                    
                    // Send session update
                    dc.send(JSON.stringify({
                        type: 'session.update',
                        session: {
                            modalities: ['text', 'audio'],
                            instructions: 'You are a helpful assistant. Respond concisely.',
                            voice: 'alloy',
                            input_audio_format: 'pcm16',
                            output_audio_format: 'pcm16',
                            temperature: 0.8
                        }
                    }));
                };

                dc.onmessage = (e) => {
                    const event = JSON.parse(e.data);
                    console.log('Event:', event.type);
                    
                    if (event.type === 'response.audio_transcript.delta') {
                        addMessage('assistant', event.delta.text);
                    }
                    if (event.type === 'conversation.item.input_audio_transcription.completed') {
                        addMessage('user', event.transcript);
                    }
                    if (event.type === 'error') {
                        console.error('Error:', event.error);
                        addMessage('error', event.error.message);
                    }
                };

                // Create offer
                const offer = await pc.createOffer();
                await pc.setLocalDescription(offer);

                // Send offer to OpenAI
                const response = await fetch('https://api.openai.com/v1/realtime/sessions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${API_KEY}`,
                        'Content-Type': 'application/sdp'
                    },
                    body: offer.sdp
                });

                if (!response.ok) {
                    throw new Error(`HTTP ${response.status}: ${await response.text()}`);
                }

                const answer = await response.text();
                await pc.setRemoteDescription({ type: 'answer', sdp: answer });

                // Set up audio
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { 
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                audioContext = new AudioContext({ sampleRate: 24000 });
                source = audioContext.createMediaStreamSource(stream);
                processor = audioContext.createScriptProcessor(2048, 1, 1);
                
                // Handle incoming audio
                pc.ontrack = (e) => {
                    const audio = new Audio();
                    audio.srcObject = e.streams[0];
                    audio.play();
                };

            } catch (error) {
                console.error('Connection error:', error);
                log('Error: ' + error.message);
                document.getElementById('connectBtn').disabled = false;
            }
        }

        function startRecording() {
            if (!dc || dc.readyState !== 'open') return;
            
            log('Recording...');
            
            source.connect(processor);
            processor.connect(audioContext.destination);
            
            processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);
                const pcm16 = new Int16Array(inputData.length);
                
                for (let i = 0; i < inputData.length; i++) {
                    const s = Math.max(-1, Math.min(1, inputData[i]));
                    pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }
                
                const base64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
                
                dc.send(JSON.stringify({
                    type: 'input_audio_buffer.append',
                    audio: base64
                }));
            };
        }

        function stopRecording() {
            if (!processor) return;
            
            log('Processing...');
            processor.disconnect();
            source.disconnect();
            
            // Commit audio and create response
            dc.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
            dc.send(JSON.stringify({ type: 'response.create' }));
            
            setTimeout(() => {
                log('Connected! Hold the button to talk.');
            }, 1000);
        }
    </script>
</body>
</html>